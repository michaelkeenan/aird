- node: ### comments in this document added by lukas, based on keenans README.md, for my own understanding & future documentation. comments starting with "#BS: " indicate how this yaml data is generated by the build script from the source google doc.
  name: Generally capable AI systems
  url: /arguments/when-agi
  effect: calculated # this means: agree/disagree state depends on children. #BS: for top-level items, this is automatically generated.
  question: When do you think these generally capable systems will exist? # "askQuestion" is not required because it defaults to true #BS: write "q:single-choice:your question?" in a separate line at the end of the section. Or use just "q:" if more than one can be selected.
  nodes:
    - node:
      name: Within 50 Years
      url: /arguments/within-50-years
      effect: agree # this means: we agree to the argument presented in that URL # BS: write "yml:effect:agree" right after the headline
      overridesSiblings: true # When you click this, its siblings are un-set (because the options are mutually exclusive) #BS: use q:single-choice on the parent
      askQuestion: false # There is no question, instead we just get the "Next: " button. #BS: use "yml:"
    - node:
      name: More than 50 years
      url: /arguments/more-than-50-years
      effect: agree
      question: Would you like to hear these arguments?
      overridesSiblings: true
      nodes:
        - node:
          name: Why these systems might come soon
          url: /arguments/agisooner
          text: Yes, I would like to hear these arguments for why AGI might come soon #BS: in the build script, we refer to a "Headline" and a "ShorterHeadline". The "Headline" equals the "text" in the yaml. The "ShorterHeadline" equals the "name" in the yaml. They are specified like this in the source document: [AGISooner|Why these systems might come soon] Yes, I would like to hear these arguments for why AGI might come soon. The ShorterHeadline is optional, in which case it is equal to the Headline, and the buildscript does not output "text:" into the yaml.
          propagateAgreement: false # this is default true. It is false here because agreement with these arguments does not automatically mean we now changed our mind on "more than 50 years". #BS: use yml: syntax
          parentListingType: button # the default is checkbox. #BS: use "q:buttons:" syntax on parent.
        - node:
          name: Moving on to potential risks
          url: /arguments/goto-potential-risk
          text: No, let’s move on - I want to learn about potential risks
          parentListingType: button
          askQuestion: false
    - node:
      name: Never
      url: /arguments/never
      question: Would you agree that there might be such generally capable systems at some time in the future?
      overridesSiblings: true
      nodes:
        - node: # this node does not correspond to an article, instead it functions like a link. #BS: use "nav:" syntax
          name: The Alignment Problem
          text: Yes, I agree there might be such generally capable systems at some time in the future (move on to potential risks from AI).
          effect: undecidedOverride # this sets its parent node to "undecided". This is because the question is phrased in such a way that it basically indicates "I now changed my mind on the parent".
          answerLinkUrl: /arguments/the-alignment-problem
          agreeTargetUrl: /arguments/never # A nodes checkbox can be on its own page (yes/no) or on its parent (checkbox on/off for a certain argument). By using this option, we can redirect the agreement/disagreement to another URL. # Why do we want this here? --> same justification as above under "effect".
          listInTree: false # same justification as above
        - node:
          name: There is something special about biology
          text: No – there is something special about biology which we will never be able to put into machines
          url: /arguments/biology-special
          question: Do you agree that biology is probably not essential for general intelligence?
        - node:
          name: Intelligent Machines - seems weird
          text: No – truly intelligent machines - that seems really weird
          url: /arguments/seems-weird
        - node:
          name: The world doesn’t change that fast
          text: No – if we actually had a “scientist AI”, it would be a massive change in the world - and the world simply doesn’t change that fast
          url: /arguments/fast-changes
        - node:
          name: AI will never be able to have true creativity
          text: No – AI will never be able to have true creativity
          url: /arguments/creativity
        - node:
          name: Would need to understand the brain first
          text: No – we would need to understand the brain first, and this is a big obstacle
          url: /arguments/understand-brain-first
        - node:
          name: I don’t see a reason for us to even want that
          text: No – I don’t see a reason for us to even want that
          url: /arguments/we-dont-want-that
        - node:
          name: Can’t see it based on current progress
          text: No – I can’t see it based on current progress
          url: /arguments/cant-see-it-based-on-current-progress
        - node:
          name: An AI paradigm shift will be required
          text: No – an AI paradigm shift will be required
          url: /arguments/an-ai-paradigm-shift-will-be-required
        - node:
          name: People would stop this
          text: No – surely this would be bad and people would stop it
          url: /arguments/surely-this-would-be-bad-and-people-would-stop-it
        - node:
          name: We need embodiment
          text: No – we need embodiment - the AI needs a body and actuators, and sensors
          url: /arguments/embodiment-is-necessary
        - node:
          name: AI cannot be conscious
          text: No – AI cannot be conscious in the way a human is
          url: /arguments/consciousness
- node:
  name: The Alignment Problem
  url: /arguments/the-alignment-problem
  effect: calculated
  nodes:
    - node: #BS: this is again a "nav:" section at the bottom of the document
      name: Agree
      linkName: Instrumental Incentives
      text: Yes, aligning AI would be difficult
      effect: agree
      nodeLinkUrl: /arguments/the-alignment-problem
      agreeTargetUrl: /arguments/the-alignment-problem
      answerLinkUrl: /arguments/instrumental-incentives
    - node: #BS: this is another "nav:" saection at the bottom of the doc, but then you add another line to it saying "add-children-here: true"
      name: Disagree
      effect: disagree
      nodeLinkUrl: /arguments/the-alignment-problem
      isCheckboxOption: false
      delegateCheckboxes: true
      nodes:
      - node:
        name: We would test before deploying
        text: No – we would test it before deploying
        url: /arguments/test-before-deploying
      - node:
        name: We would just be careful with our reward function
        text: No – we would just be careful with our reward function
        url: /arguments/careful-with-that-reward-function
      - node:
        name: AGI wouldn’t be functional unless this problem is solved
        text: No – AGI wouldn’t be functional unless this problem is solved
        url: /arguments/wouldnt-be-functional
      - node:
        name: This will probably be solved in the course of building these systems
        text: No – this will probably be solved in the course of building these systems
        url: /arguments/would-inevitably-be-solved
      - node:
        name: Our understanding of how to align systems will advance
        text: No – our understanding of how to align systems will advance
        url: /arguments/alignment-advances-equally
      - node:
        name: Need to know what type of AGI before we can talk about safety
        text: No – we need to know what type of AGI before we can talk about safety
        url: /arguments/need-to-know-what-type
      - node:
        name: Misuse is a bigger problem
        text: No – misuse is a bigger problem
        url: /arguments/misuse-is-a-bigger-problem
      - node:
        name: This is not as dangerous as other global risks
        text: No – this is not as dangerous as other global risks
        url: /arguments/this-is-not-as-dangerous-as-other-global-risks
      - node:
        name: Humans have alignment problems too
        text: No - humans have alignment problems too
        url: /arguments/humans-have-alignment-problems-too
- node:
  name: Instrumental Incentives
  url: /arguments/instrumental-incentives
  effect: calculated
  nodes:
    - node:
      name: Agree
      linkName: Threat Models
      text: Yes, the prospect of an AGI seizing power and resisting shutdown is a serious problem
      effect: agree
      nodeLinkUrl: /arguments/instrumental-incentives
      agreeTargetUrl: /arguments/instrumental-incentives
      answerLinkUrl: /arguments/threat-models
    - node:
      name: Disagree
      effect: disagree
      nodeLinkUrl: /arguments/instrumental-incentives
      isCheckboxOption: false
      delegateCheckboxes: true
      nodes:
      - node:
        name: It would be simple to instruct the AI to try less hard
        text: No – it would be simple to instruct the AI to try less hard
        url: /arguments/aishould-try-less-hard
      - node:
        name: Consciousness won’t happen
        url: /arguments/no-self-preservation-without-consciousness
      - node:
        name: We could stop that physically
        url: /arguments/we-could-stop-that-physically
      - node:
        name: Current systems don’t do that
        url: /arguments/current-systems-dont-do-that
      - node:
        name: We wouldn’t design it that way
        url: /arguments/wouldnt-design-that-way
      - node:
        name: This won’t be a problem, we will have human oversight
        url: /arguments/human-oversight
      - node:
        name: AI could be a big deal
        url: /arguments/ai-could-be-a-big-deal
- node:
  name: Threat Models - how might AGI be dangerous?
  url: /arguments/threat-models
  effect: calculated
- node:
  name: The importance of more safety work
  url: /arguments/the-importance-of-more-safety-work
  effect: calculated
  nodes:
    - node:
      name: Agree
      text: Yes, safety work is important
      effect: agree
      nodeLinkUrl: /arguments/the-importance-of-more-safety-work
      agreeTargetUrl: /arguments/the-importance-of-more-safety-work
      answerLinkUrl: /arguments/conclusion
    - node:
      name: Disagree
      effect: disagree
      nodeLinkUrl: /arguments/the-importance-of-more-safety-work
      isCheckboxOption: false
      delegateCheckboxes: true
      nodes:
      - node:
        name: Regulators will take care of this
        url: /arguments/regulators-will-take-care-of-this
      - node:
        name: Nothing we can do at the moment
        url: /arguments/not-useful-currently
      - node:
        name: I’m not convinced this is urgent now
        url: /arguments/not-urgent-currently
      - node:
        name: Lets build AI, worry about safety later
        url: /arguments/lets-build-worry-later
      - node:
        name: Alignment is easy - This will definitely be solved in time
        url: /arguments/alignment-is-easy-this-will-definitely-be-solved-in-time
      - node:
        name: I don’t see how current safety work is useful
        url: /arguments/not-currently-tractable
- node:
  name: Conclusion
  effect: calculated
  url: /arguments/conclusion
