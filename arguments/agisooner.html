---
layout: argument
title: "Yes, I would like to hear these arguments for why AGI might come soon"
breadcrumbs: When will we get generally capable AI systems?:when-agi,More than 50 years:more-than-50-years,Why these systems might come soon:agisooner
---
<div>here’s a textblock from the transcripts I’m not quite sure what to do with at this point: In 2012, the deep learning revolution started with AlexNet and GPUs. And here we are 10 years later - with systems like GPT-3, which have weirdly emerging capabilities. They can do some text generation and some translation and some coding and some math. One can think that if one continues pouring in all of the human investment that is currently being poured into AI in terms of talent, money, etc, and then training data, and compute, if we see algorithmic improvements the same rate that we've seen, and if we see hardware improvements, also maybe we'll get optical computing or quantum computing, then maybe we reach something like AGI or AI at very high levels of capabilities. Or maybe we don't. We hit some ceiling. And then we do a paradigm shift and the next paradigm might get us there.</div>
<ol><li>In a <a href=' https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/#Summary_of_results '>2022 survey of AI researchers</a>, the aggregate forecasted time until human-level AI was 37 years (2059). A <a href=' https://www.cold-takes.com/are-we-trending-toward-transformative-ai-how-would-we-know/#surveying-experts '>survey from 2017 </a> asked for ““when unaided machines can accomplish every task better and more cheaply than human workers" and got 20% probability by 2036, 50% by 2060, and 70% by 2100.</li>
<li>Compute Trends & Scaling Hypothesis</li>
<ol><li>Significant investment into deep learning, with the involvement of GPUs and massive scaling, started around 2009. It's worth noting that we've been working on AI for less than 100 years and the current paradigm is around 10 years old, and that we've gotten pretty far in that time. However, some people think that we should be much further.</li>
<li>We haven’t worked on this for very long, with AI research starting in the 1950s and <a href=' https://venturebeat.com/ai/report-ai-investments-see-largest-year-over-year-growth-in-20-years/ '>investments increasing massively</a> since 2010.</li>
<li>Data from <a href='https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf'>PaLM</a>, the most compute intensive model ever trained at that time, suggests that improvements continue scaling well with model size even in the 500+ billion parameter range. New capabilities emerge just as a function of scale: For example, PaLM is able to explain novel jokes. It also surpassed previous, smaller models on 28 out of 29 widely-used NLP tasks.</li>
<li>Since then, the amount of compute invested into training large ML systems has increased tremndously - with a <a href=' https://ar5iv.labs.arxiv.org/html/2202.05924 '>5.7 month doubling time since 2022</a>. According to that paper, the growth rate for the largest models is somewhat slower, but still significantly faster than Moores Law would suggest, at 10.7 months.</li>
<li>Economic incentives and international competition motivates ongoing growth in investment.</li>
</ol><li>The forecasting platform Metaculus estimates <a href=' https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/ '>the arrival of general AI in 2042</a>.</li>
<li>A <a href=' https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines '>forecasting model</a> based on the amount of compute performed in biological systems estimated the year 2052, with a plausible range of 2040-2090. (<a href=' https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/ '>more discussion including some criticism of that approach here</a>)</li>
</ol><div>Further Reading:</div>
<ul><li><a href=' https://docs.google.com/document/d/1j7tZ1Xf7-l2k2qr2t3MFwi-IkhXNdzA2N2WZBfcghsM/edit# '>A list of researcher surveys on AI timelines and risks</a></li>
<li>The AI Timelines post by Holden Karnofsky, blog series and podcast.</li>
