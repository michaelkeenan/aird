---
layout: argument
title: "Our understanding of how to align systems will advance"
breadcrumbs: The Alignment Problem:the-alignment-problem,Our understanding of how to align systems will advance:alignment-advances-equally
---
<p>Some researchers There’s an argu e ment that alignment advances will proceed at the same pace as capabilities advances. If “ alignment ” is trying to have an AI system be aligned with human goals, then isn’t that also the point of “capabilities”, or advancing AI in general? Would they not proceed apace?</p>
<ul><li>This is definitely a possibility! Maybe capabilities would be directly tied to alignment in some way. Maybe the development of advanced AI systems will go just fine.</li>
<li>However, there is the distinct possibility that things will not be fine. One reason for that is instrumental incentives : An advanced AI system, regardless of its goals, will probably have certain incentives relating to its own capabilities. It might, for example, employ strategies to avoid it from being switched off, and try to get access to more comput ational resources e . We will cover this possibility in a later chapter.</li>
</ul>
<blockquote>
I'm actually far less worried about the technical side of this. I just finished reading this book about von Neumann, that's a little cute biography of him, and there's a part where he says, supposedly, that people who think mathematics is complicated only say that because they don't know how complicated life is. And I'm totally messing with the phrasing, but something like that. I actually think any technical problems in this area will be solved relatively easily compared to the problem of figuring out what human values we want to insert into these. 
</blockquote>

 <li>Due to economic incentives, new technologies Economic incentives mean things are often deployed without adequate confirmation of safety being perfectly safe , especially if people don’t think it needs to be super carefully checked.</li>
<li>Before the first test of the atomic bomb, there were some concerns that an energy release of this magnitude might cause a chain reaction within the atmosphere - turning the surface of earth into a barren wasteland. Further calculations were done and showed that this is not possible. However, the calculation was only checked by a small number of researchers and it could have been possible, though unlikely, that there was a mistake in the calculation or the underlying assumptions. <a href='https://blogs.scientificamerican.com/cross-check/bethe-teller-trinity-and-the-end-of-earth/' target='_blank'>Further reading</a></li>
<li>Current AI systems have significant ethical problems, and some of these problems are only discovered in production.</li>
<li>Often, it is not possible to fix all these problems (for example, ChatGPT can be instructed to write toxic material despite efforts at preventing that)</li>
<li>It is possible that an AGI system would behave reasonably well in testing , but then produce catastrophic results in the real world.<br/></li>
<li><li>An AGI system, in order to have general intelligence, will probably need to be aware about its own inner workings and its place in the world. This would mean it might be able to know that it’s being trained, and it could intentionally display safe behavior in training, hiding its true intentions until later. This sounds far-fetched, but is a natural consequence of assuming that future systems will have high amounts of intelligence and an accurate model of the world. </li>
<li>Maybe alignment will be solved during the normal course cause of AI research , - but maybe not! We cannot say that for certain, given what we see with current technologies - some of them turn out to be quite safe inherently, others are risky even if properly managed.</li>
<li>During the Cold War, the RAND Corporation had to frantically work out the underlying game theoretical logic of nuclear warfare while actively threatened by the risk of global catastrophe. Though we managed to avoid disaster, it would have been better if we could have worked out the appropriate strategies and decision-making systems before nuclear weapons technology matured. With AGI, we have the chance to do that work in advance, and the risk seems serious enough that we should do it unless we can think of very good reasons not to.</li>
<li>Alignment and Capabilities are not the same. You can have very advanced systems with bad alignment, for example a GPT-style text generator that seems really smart - but constructs elaborate falsehoods that seem true at first glance.</li>
<li>Even if it seems likely that alignment will advance alongside capabilities, we don’t have any guarantee about that. Notable experts in the field, such as Eliezer Yudkowsky and Nick Bostrom, claim that the The danger from a badly aligned system s are alarmingly seems really high . - so it’s probably worth worrying about that.</li>
<li>Safety often lags behind in the deployment of innovative technology. <li>There is a long history of dangerous technology being deployed prematurely, before the technology was understood well enough to be safe. Some examples:</li>
<ul><li>Radioactive toothpaste being sold in Germany in the 1920s</li>
<li>Early passenger aircraft (like the de Havilland Comet ) had rectangular windows, before it was understood that rectangular features put too much stress on the airframe - leading to several disasters.</li>
<li>The Therac-25 radiation treatment machine which killed several people due to a programming error.</li>
<li>The dangerous reactor design which contributed to the Chernobyl disaster.<br/></li>
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
