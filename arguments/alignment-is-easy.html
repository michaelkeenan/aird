---
layout: argument
title: "Alignment is easy"
breadcrumbs: The Alignment Problem:the-alignment-problem,Alignment is easy:alignment-is-easy
---
<p>The alignment problem might seem easy to solve at first glance. However, as more and more people have researched it, we have uncovered a wide range of problems, including some which might not be obvious at first glance.</p>
For example: <li>An AGI system, in order to have general intelligence, will probably need to be aware about its own inner workings and its place in the world. This would mean it might be able to know that it’s being trained, and it could intentionally display safe behavior in training, hiding its true intentions until later. This sounds far-fetched, but is a natural consequence of assuming that future systems will have high amounts of intelligence and an accurate model of the world.
<p>Obviously, much of this research is conceptual and we do not have any actual AGI systems to test our hypotheses on. That being said, a lot of these insights are independent of the particulars of AI architecture.</p>
<p>Furthermore, even if the alignment problem could be solved easily, it is not clear that the solution will be found in time:</p>
<ul><li>There is a significant resource gap between work on safety, compared to work on capabilities. [copy.HowManyPeopleWorkInSafety]</li>
</ul><p>And even if a solution is found in time, it might not be implemented correctly:</p>
<ul><li><li>Due to economic incentives, new technologies  are often deployed without adequate confirmation of safety, especially if people don’t think it needs to be carefully checked.</li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
