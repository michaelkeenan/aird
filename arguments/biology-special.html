---
layout: argument
title: "There is something special about biology which we will never be able to put into machines"
breadcrumbs: When will we get generally capable AI systems?:when-agi,Never:never,There is something special about biology which we will never be able to put into machines:biology-special
---
<div><blockquote>There is certainly something special about biology. We still cannot replicate many biological systems. (made up)</blockquote></div>
<div>Consider that current AI systems can do things that would have seemed magical just a few years ago. For example:</div>
<ul><figure><img src='https://lh3.googleusercontent.com/xr3JVvcZWzwFFNn82enZN1UzslHNt6geTqaLEaTHjBe01fwAZaY6hjYgC7-AuIH1jkfWFy_huEhdkasg0txp6XmPLkohHVrcwLzahNUYsJWUAht5e4zXd4YshZb8Cds9cufyZKcOTkn23Ln1XouDriTKH_FSk_qTR43rf9WYMI3IciNYImowdyyutpKDEcfobNw' referrerpolicy='no-referrer'/><figcaption markdown='1'>Image generated by DALL-E 2 based on the text prompt “teddy bears shopping for groceries in ancient Egypt”. DALL-E (2021) generates images based on text prompts while exhibiting what could be reasonably described as “true creativity”
</figcaption></figure>
<figure><img src='https://lh4.googleusercontent.com/B8tDvsYvSJ1QqzpH9uGQILlWc9Dc9f_pUfieNR4UYSbkjPmPGcK35mj1ZaEBp6lk7ZrbaXycBjGQlQj-XOAzXPr0WKh57jOYEyrMLPKGlgrVs_Yrm5OTeJisWkay884-TnhPBuFjBoCcfG-M2M3Q02HgkzSdaKJ0Azo0i5nztmp8QNul22iMSQS8COK9Ihah6yw' referrerpolicy='no-referrer'/><figcaption markdown='1'>Google Imagen (2022) produces even more realistic depictions of even wilder scenarios. This is “Sprouts in the shape of text 'Imagen' coming out of a fairytale book.”
</figcaption></figure>
<li>GPT-3 (2020) is able to write convincing articles, generate code based on natural language, and perform well on a wide range of text-based tasks.</li>
<li>PaLM (2022) improving upon GPT-3 and other state-of-the-art systems mostly by increasing the number of parameters and training on better hardware - without revolutionary new insights into the nature of human intelligence.</li>
<ul><li>The system improves upon the state-of-the-art in 28 of 29 tested NLP tasks.</li>
<li>It is able, among other things, to explain an original joke in two-shot prompts.</li>
<li>In code generation, it equals the performance of Codex 12B (which was fine-tuned on that task) while using 50 times less Python code.</li>
