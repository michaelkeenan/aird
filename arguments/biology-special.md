---
layout: argument
title: "There is something special about biology which we will never be able to put into machines"
breadcrumbs: When will we get generally capable AI systems?:when-agi,Never:never,There is something special about biology which we will never be able to put into machines:biology-special
---
<blockquote>There is certainly something special about biology. We still cannot replicate many biological systems. (made up)</blockquote>
Consider that current AI systems can do things that would have seemed magical just a few years ago. For example:
<ul><figure><img src='https://lh6.googleusercontent.com/OnhA0fhk2wp5ZKc_vnu-YOkhSkPETBUG4RpKsMzSSdt0GUow5GY84lpFXYxR4xgPYC_YOfsC9CnYgGZ_p5O2zmI7p9n4D3b-hq-8qXYTqACL_pfjKGLpbJovT6ZM8Ku6-QIYRkFeXYEKBEpkYdRcbTmWM77A01lgL9mRi2_Q4i3nuDvTk-abVm06e3LLfVmk5xA' referrerpolicy='no-referrer'/><figcaption markdown='1'>Image generated by DALL-E 2 based on the text prompt “teddy bears shopping for groceries in ancient Egypt”. DALL-E (2021) generates images based on text prompts while exhibiting what could be reasonably described as “true creativity”
</figcaption></figure>
<figure><img src='https://lh6.googleusercontent.com/54auz1O37hlIJ1KwdtUQrRMqUjIbRQxWwWTXemy15zs-5zLlfR68bVVRSkBYNb6anj-HzGBPTov1Y9AkKCni7h23fbNIdz0ZpyDx18ZuNQqn0LIo1q_Q-HnSnAGco38btZ5zPwnsy2XrB-Rek8uf_8jef4dYZ2PIU1r-W4xBbN0RHqbNTg0tOXdfCudCzHAQuaI' referrerpolicy='no-referrer'/><figcaption markdown='1'>Google Imagen (2022) produces even more realistic depictions of even wilder scenarios. This is “Sprouts in the shape of text 'Imagen' coming out of a fairytale book.”
</figcaption></figure>
<li>GPT-3 (2020) is able to write convincing articles, generate code based on natural language, and perform well on a wide range of text-based tasks.</li>
<li>PaLM (2022) improving upon GPT-3 and other state-of-the-art systems mostly by increasing the number of parameters and training on better hardware - without revolutionary new insights into the nature of human intelligence.</li>
<ul><li>The system improves upon the state-of-the-art in 28 of 29 tested NLP tasks.</li>
<li>It is able, among other things, to explain an original joke in two-shot prompts.</li>
<li>In code generation, it equals the performance of Codex 12B (which was fine-tuned on that task) while using 50 times less Python code.</li>
