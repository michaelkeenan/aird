---
layout: argument
title: "AI cannot be conscious"
breadcrumbs: Generally capable AI systems:when-agi,Never:never,AI cannot be conscious:consciousness
---

<blockquote>
  I don't see we can ever get to a point where these models are sort of conscious or know what they're thinking about or are able to take on these type of challenging tasks where you have a CEO, neural network or something. 
</blockquote>

<div>There is an argument that AI cannot be conscious (in the sense of having felt experience), and therefore it cannot act intelligently.</div>
<div>How consciousness arises and whether a computer program can ever be conscious are open questions. It is unclear if consciousness (in the sense of “felt experience”) is required for AGI.</div>
<div>If an AI is conscious, it would have profound and likely troubling implications. However, for the discussion here it’s not important whether or not systems are conscious. We are discussing potential risks that arise out of certain capabilities in advanced systems. The arguments do not require assumptions on whether or not that system would be conscious - all we are concerned about here is the behaviour.</div>
<div>We consider the potential risk of these advanced AI systems severe enough (potential existential risk) to discuss even without the added complication of consciousness arising in these systems.</div>
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
