---
layout: argument
title: "Current systems don’t do that"
breadcrumbs: Instrumental Incentives:instrumental-incentives,Current systems don’t do that:current-systems-dont-do-that
---

<blockquote>
I feel like I don't see why this would ever happen or how this would ever happen in a model, simply because it's not what it was intended to do, and I feel like going out of that scope of initial tasks rarely happens. 
</blockquote>

<ul><li>Current AI systems won’t act in that way, but future systems might.</li>
<li>In fact, given certain assumptions, we would expect that systems act in accordance with certain instrumental incentives. All that is necessary to get self-preservation is</p>
<ol><li>that the system has knowledge of itself and about how it could influence the world</li>
<li>that the system does advanced planning and is able to find ways of action that maximize the chance it will achieve its goals.</li>
</ol></li>
<li>Recent systems like Google PaLM have demonstrated sudden capability gain upon increases in model size. It is difficult to say which additional capabilities could arise in the future as a result of that.</li>
<li>In a NeurIPS poster from 2021 (<a href='https://neurips.cc/virtual/2021/poster/28400' target='_blank'>video</a>, <a href='https://arxiv.org/pdf/1912.01683.pdf' target='_blank'>PDF paper</a>), Turner et al develop a formal theory of statistical tendencies of optimal processes. From the abstract: “To clarify this discussion, we develop the first formal theory of the statistical tendencies of optimal policies. In the context of Markov decision processes (MDPs), we prove that certain environmental symmetries are sufficient for optimal policies to tend to seek power over the environment. These symmetries exist in many environments in which the agent can be shut down or destroyed. We prove that in these environments, most reward functions make it optimal to seek power by keeping a range of options available and, when maximizing average reward, by navigating towards larger sets of potential terminal states.”</li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
