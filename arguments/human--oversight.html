---
layout: argument
title: "This won’t be a problem, we will have human oversight"
breadcrumbs: Instrumental Incentives:instrumental--incentives,Human Oversight:human--oversight
---
<div><blockquote></div>
<div>“Ok, an AI system might try to prevent itself from being switched off. But it won’t succeed, and we will uncover the deception” (made up)</div>
<div>“This could happen in theory, but would not happen in practice - surely we will oversee its operation and find a way to turn it off if necessary” (made up)</blockquote></div>
<div>Current ML models are, in many cases, black boxes. There is a lot of work going into interpretability, and it is likely we will get better at understanding these models. That being said, we are far from having clear insight into large state-of-the-art models. If research on AI capabilities continues progressing, it is likely that when TAI arrives, we will not yet have the ability to truly understand what it does internally. Hypothetically, if the TAI system has enough insight into its own inner workings, it might even rewrite itself in a way that makes it even more difficult to understand.</div>
<div>To avoid itself being switched off, a future AI system would have a wide range of options:</div>
<ul><li>Hide information that would cause human operators to be skeptical about the systems next steps</li>
<li>Convince or bribe its human operators</li>
<li>Display human-approved, conservative behaviour. Then wait for the point where the system has been deployed to many different geographically separated machines.</li>
<li>Gain access to servers on the internet and run copies on them. (Even average human hackers achieve this regularly)</li>
</ul><div>An AGI could be smarter than a human, which means it might out-think its operators, anticipate all kinds of ways in which it could be stopped - and design strategies against that.</div>
<div>If the AI is incentivized against its operators, we are basically doomed. The way to stop it is not to see what it does and then react by trying to shut it down. Instead, we must find a way to design it so that it is aligned with our interests.</div>
<div>The real world contains a very wide action space. Shoring up one part of that action space really does not help against a powerful malevolent AI.</div>
<div>IT security is far from a solved issue, current human-designed IT systems often have security holes that other humans can find. A human-designed sandbox for an AI has probably some security holes that a smarter-than-human AI could identify and exploit.</div>
<div>Sidenote: Even if we design it to be aligned with our interests, we should still make sure we have good safety measures in place - because there are always unexpected failure modes with tech.</div>
<div>L: Is this list too wild?<br/>V:</div>
<div>I think it's basically fine. my usual points here:</div>
<div>- this thing is by definition superintelligent, means it can OUT-THINK you in any way you can think to stop it (some people really don't grok this, not sure how to push it into their system)</div>
<div>- relatedly, the problem is if the AI is incentivized against you at all, in which case you're doomed. The solution is not (shore it up better) if you are in that world, the solution is (make sure it's not incentivized against you). In different worlds where the AI isn't incentivized against you, then you should (shore things up better) since we're operating within normal tech which has normal failure modes and normal human misuse.</div>
<div>- security is a thing</div>
<div>- human failure is a thing</div>
<div>- human misuse is a thing</div>
<div>- embodiment is something the AI can do</div>
<div>- THE INTERNET is something AI can do</div>
<div>- general point is that there’s a very wide action space here, shoring up on one part of the action space doesn’t help you against an all-powerful malevolent AI</div>
