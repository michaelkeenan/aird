---
layout: argument
title: "Instrumental Incentives"
breadcrumbs: Instrumental Incentives:instrumental--incentives
---
<ul><li>We now agree that there might be something like a “CEO AI” that’s running a company, and that it is difficult to put the goals we would want it to have into mathematical formulations.</li>
<li>There’s another problem that makes formulating goals for highly capable AI systems difficult. Suppose we have told the CEO AI that it should optimize for some goal. And then after a few days we find a problem and switch it off for maintenance. When we restart it, it would realize that some of its plans are failing because it keeps getting switched off.</li>
<li>So it would try to find a way to prevent that from happening. For example, it could omit critical information which its human operators might find objectionable from its next report.</li>
<li>Such behaviour does not need to be programmed in. It would emerge under certain conditions: If the system is capable enough, models the world reasonably well and optimizes for almost any goal, it will want to prevent attempts to be shut down.</li>
<li>This does not require the AI system to care about “itself” in the sense that humans care about their own survival. All it requires is the understanding that its goals are unachievable if the system is turned off.</li>
<li>Furthermore: In order to solve problems faster and achieve its goals with higher likelihood, the AI system will try to gather resources and gain more power.</li>
</ul>If this were the case, we wouldn’t build such systems in the first place: goto [Economic Incentives]
Yes, I agree this can be a problem at least in theory: goto [Danger Scenarios]
<div><a href='/arguments/it-would-be-simple-to-instruct-the--ai-to-try-less-hard.html'>It would be simple to instruct the AI to try less hard</a></div>
<div><a href='/arguments/consciousness-won’t-happen-(and-is-required-for-self-preservation).html'>Consciousness won’t happen (and is required for self-preservation)</a></div>
<div><a href='/arguments/we-could-stop-that-physically.html'>We could stop that physically</a></div>
<div><a href='/arguments/current-systems-don’t-do-that.html'>Current systems don’t do that</a></div>
<div><a href='/arguments/we-wouldn’t-design-it-that-way.html'>We wouldn’t design it that way</a></div>
<div><a href='/arguments/human--oversight.html'>This won’t be a problem, we will have human oversight</a></div>
<div><a href='/arguments/ai-could-be-a-big-deal.html'>AI could be a big deal</a></div>
