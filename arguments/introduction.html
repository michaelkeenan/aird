---
layout: argument
title: "Introduction"
breadcrumbs: Introduction:introduction
---
<p>Welcome to this interactive guide on the potential risks from advanced AI systems. We hope this will help you explore some of the ongoing discussion on the development and deployment of highly capable AI systems.</p>
<p>As you go through this guide, you'll encounter a range of arguments and counterarguments related to the potential risks of advanced AI. At the bottom of each page, you'll have the opportunity to select between several answers or counterarguments.</p>
<p>The arguments and counterarguments presented in this guide are based on interviews with 97 AI researchers (92 randomly selected from submissions to NeurIPS or ICLR 2021; 5 selected by recommendation), and reflect the most common perspectives within that population. Embedded quotes are from those interviews. We welcome your feedback and encourage you to get in touch with us, especially if your own views are not represented here. Additionally, if you have any other comments or questions as you go through this interactive guide, please contact us via the feedback button at the bottom of each page.</p>
<p>At times we will entertain thought experiments that would be implausible with current state-of-the-art systems, but might be relevant to future systems. These types of thought experiments often blur the lines between philosophy and computer science. <a href='https://bounded-regret.ghost.io/more-is-different-for-ai/' target='_blank'>Jacob Steinhardt</a> has written about this tension between the “engineering approach” and the “philosophy approach”. He argues that leaning on both of these approaches is more helpful than using just one of them. Therefore, in this guide we aim for a balanced picture, by supplementing our thought experiments with real-world examples.</p>
<p>Jacob Steinhard explains:</p>

<blockquote><p>
I’ll argue that:</p>
<ol><li>Future ML Systems will be qualitatively different from those we see today. Indeed, ML systems have historically exhibited qualitative changes as a result of increasing their scale. This is an instance of "More Is Different", which is commonplace in other fields such as physics, biology, and economics . Consequently, we should expect ML to exhibit more qualitative changes as it scales up in the future.</li>
<li>Most discussions of ML failures are anchored either on existing systems or on humans. Thought Experiments Provide a Third Anchor, and having three anchors is much better than having two, but each has its own weaknesses.</li>
<li>If we take thought experiments seriously, we end up predicting that ML Systems Will Have Weird Failure Modes. Some important failure modes of ML systems will not be present in any existing systems, and might manifest quickly enough that we can't safely wait for them to occur before addressing them.<br/>
</blockquote>
</li>
</ol><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
