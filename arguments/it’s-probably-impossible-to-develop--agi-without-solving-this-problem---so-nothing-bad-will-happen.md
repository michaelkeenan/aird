---
layout: argument
title: "It’s probably impossible to develop AGI without solving this problem - so nothing bad will happen"
breadcrumbs: The Alignment Problem:the--alignment--problem,It’s probably impossible to develop AGI without solving this problem - so nothing bad will happen:it’s-probably-impossible-to-develop--agi-without-solving-this-problem---so-nothing-bad-will-happen
---
<blockquote>The premise doesn’t make sense (How are we going to put everything that we care about into one reward function? We don’t know how to do that.), which means we’ll never do it, and Perhaps that’s good news? Perhaps that would mean it is impossible to develop AGI without solving this problem? Perhaps they just wouldn’t be functional? (so nothing bad will happen) (made up)</blockquote>
<ul><li>That seems unlikely. Why would the AGI be nonfunctional unless the goals encompassed “everything we care about”? Surely we are going to try and build more and more complex AI systems, with various goals.</li>
<li>The goals will get more complex - and we might not know what we’re doing as we scale this up.</li>
