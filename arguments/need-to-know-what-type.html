---
layout: argument
title: "Need to know what type of AGI before we can talk about safety"
breadcrumbs: The Alignment Problem:the--alignment--problem,Need to know what type of AGI before we can talk about safety:need-to-know-what-type
---
<div><blockquote>How did the AGI come about, I think it's pretty difficult. My intuition, that could be wrong, is that it's pretty difficult to reason about how to make it safe if we don't know what the AGI... If the AGI is a deep neural network or is a set of logic rules that are somehow just chained together into a more and more complicated loop that results in AGI or something. (na5j9_VaelLabel, Pos. 155)</div>
<div></blockquote></div>
<ul><li>If AGI comes through entirely different paradigms than current AI systems, that might mean we cannot have much insight into what makes it safe.</li>
<li>However, it might be that AGI comes soon, and it might come through existing paradigms.</li>
<ul><li>Consider how much progress has been made simply by scaling up existing approaches, and how the benefits of scaling just seem to continue even into the >500 billion neuron range.</li>
<li>Also, consider the sudden emergence of new capabilities in language models as scale increases.</li>
</ul><li>Regardless of that, it seems worth keeping an eye on alignment - even if AGI does not come through existing paradigms. While the specifics of the problem might be viewed in a different light in the future, we can already foresee that it can be a problem and thus should start paying attention now.</li>
<li>It is difficult to reason about a hypothetical future kind of technology that has never existed before For example, people in the 1950 could not have foreseen the deep learning revolution..</li>
<li>Thatâ€™s why we make sure to focus our arguments on plausible ways in which any agent might behave - regardless of the kind of technology that was used to build it.</li>
