---
layout: argument
title: "Let’s test before deploying"
breadcrumbs: The Alignment Problem:the-alignment-problem,Let’s test before deploying:test-before-deploying
---

<blockquote>
If we are saying that the whole system is designed perfectly and it's us who are feeding in the wrong goals, then isn't it the human's problem that they then try to use it correctly? And if the system is not designed correctly, then we should have first tested it to make sure that it doesn't go towards the edge cases and start harming us. If humans are the ones who design the system, they should know at least when it will work and some cases when it won't work. And we should have created some kind of a test. And also some cases like you cannot go beyond this, if you go beyond this and try to do this, we shut you off. 
</blockquote>

<p>AI systems frequently fail when encountering out-of-distribution data. By necessity, an AGI in the real world would often operate far outside its training envelope.</p>
An AI system can be deceptively aligned, i.e. display safe behaviour in the training environment but then do the wrong thing in the testing environment. This sounds far-fetched, but is a natural consequence of assuming that future systems will have high amounts of intelligence and an accurate model of the world.
<ul><li>An AI employing deception may like a far-fetched scenario. However, forms of deception have already been observed even in relatively simple systems: In <a href='https://arxiv.org/abs/1803.03453' target='_blank'>The Surprising Creativity of Digital Evolution</a>, Ofria recounts their experience from a 2001 simulated biology study published in Nature. In this study, evolutionary programming led to organisms being able to detect whether or not they are in the training environment, and radically change their behaviour based on that. This is an example of deception arising from training pressures, without even requiring general intelligence.</li>
<li>In <a href='https://arxiv.org/abs/2105.14111' target='_blank'>Goal Misgeneralization in Deep Reinforcement Learning</a> (ICML 2022), Longosco et al provide the first empirical demonstrations of goal misgeneralization: In a simple toy environment, an agent learns the wrong objective due to slight distributional shifts between training and testing environment. This system is not an AGI with an intention to deceive - nevertheless, the outcome is the same: The system displays safe behaviour in training but goes for the wrong goal in the testing environment.</li>
<li>If we look at a hypothetical AGI system, it is conceivable that it would form an intent to deceive. The reason is that in order to have general intelligence, will probably need to be aware about its own inner workings and its place in the world. This world model will include the AI itself as part of that world model, and as an object of its own decisions. That means, whether or not it gets deployed will influence the world and the goals the AI cares about. It might recognize that it will not get deployed unless it behaviour in the training environment is satisfactory. So “satisfactory behaviour in the training environment” might be something the AI explicitely optimizes for, while exhibiting different behaviour in the testing environment.</li>
<li>This is a complex subject, here are additional resources:</li>
<ul><a href='https://www.alignmentforum.org/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment' target='_blank'><li>How likely is deceptive alignment</a> - presentation by Evan Hubinger.</li>
<a href='https://arxiv.org/abs/2210.01790' target='_blank'><li>Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals</a> - 2022 preprint by Rohin Shah (DeepMind) et al. They provide several examples for goal misgeneralization in present-day deep learning systems.</li>
<li>The ICML paper mentioned above was popularized in a <a href='https://www.youtube.com/watch?v=zkbPdEHEyEI]' target='_blank'>YouTube video by AI researcher Robert Miles</a>.<br/></li>
</ul></ul>There is a long history of dangerous technology being deployed prematurely, before the technology was understood well enough to be safe. Some examples:
<ul><li>Radioactive toothpaste being sold in Germany in the 1920s</li>
<li>Early passenger aircraft (like the de Havilland Comet ) had rectangular windows, before it was understood that rectangular features put too much stress on the airframe, leading to several disasters.</li>
<li>The Therac-25 radiation treatment machine which killed several people due to a programming error.</li>
<li>The dangerous reactor design which contributed to the Chernobyl disaster.<br/></li>
</ul><p>Additionally, if the use of a technology could result in wide-scale negative outcomes, testing procedures need to be exceptionally thorough to prevent such outcomes, since unilateral action from the more careless/clueless or ill-intentioned user could result in disaster orcompounding errors, as was true of e.g. <a href='https://youtu.be/_ptmjAbacMk?t=799' target='_blank'>the Bhopal disaster, as explained in this analysis</a>. It would not be surprising if testing was not sufficiently thorough to protect from these worst-case scenarios.</p>
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
