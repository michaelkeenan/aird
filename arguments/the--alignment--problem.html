---
layout: argument
title: "The Alignment Problem"
breadcrumbs: The Alignment Problem:the--alignment--problem
---
<ul><li>Current ML systems can fail in surprising and unexpected ways. <a href=' https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml '>Many examples have been collected</a></li>
<li>Suppose we have a future “CEO AI” that’s running a company. Their board of directors might want to give the AI the goal: “Maximize profits without exploiting people, don’t run out of money, and avoid side effects that people would consider objectionable”.</li>
<li>Currently we find it very challenging to translate these human values, preferences and intentions into mathematical formulations that can be optimized by systems. This might continue to be a problem in the future.</li>
</ul><div>Would you agree that highly intelligent systems might fail to optimize exactly what their designers intended them to, and this is dangerous?</div>
<div><a href='/arguments/test-before-deploying.html'>It’s no problem, we would test it before deploying</a></div>
<div><a href='/arguments/we-shut-it-down.html'>It’s no problem: if it misbehaves, we shut it down</a></div>
<div><a href='/arguments/careful-with-that-reward-function.html'>It’s no problem, we would just be careful with our reward function</a></div>
<div><a href='/arguments/it’s-probably-impossible-to-develop--agi-without-solving-this-problem---so-nothing-bad-will-happen.html'>It’s probably impossible to develop AGI without solving this problem - so nothing bad will happen</a></div>
<div><a href='/arguments/this-problem-would-be-inevitably-solved-in-order-to-even-develop--agi.html'>This problem would be inevitably solved in order to even develop AGI</a></div>
<div><a href='/arguments/alignment-advances-equally.html'>But as we build more capable systems, surely our understanding of how to align them will advance equally well</a></div>
<div><a href='/arguments/need-to-know-what-type-of--agi-before-we-can-talk-about-safety.html'>Need to know what type of AGI before we can talk about safety</a></div>
<div><a href='/arguments/misuse-is-a-bigger-problem.html'>Misuse is a bigger problem</a></div>
<div><a href='/arguments/this-is-not-as-dangerous-as-other-global-risks.html'>This is not as dangerous as other global risks</a></div>
<div><a href='/arguments/humans-have-alignment-problems-too.html'>Humans have alignment problems too</a></div>
<div><a href='/arguments/instrumental--incentives.html'>Instrumental Incentives</a></div>
