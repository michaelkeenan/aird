---
layout: argument
title: "The Alignment Problem"
breadcrumbs: The Alignment Problem:the-alignment-problem
---
<ul><li>Current ML systems can fail in surprising and unexpected ways. <a href='https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml' target='_blank'>Many examples have been collected</a></li>
<li>Suppose we have a future “CEO AI” that’s running a company. Their board of directors might give the AI the goal: “Maximize profits without exploiting people, don’t run out of money, and avoid side effects that people would consider objectionable”.</li>
<li>Currently we find it very challenging to translate these human values, preferences and intentions into mathematical formulations that can be optimized by systems. This might continue to be a problem in the future.</li>
<li>Mistakes in goal formulation could be dangerous and have far-reaching consequences. For example:</li>
<ul><li>The AI might optimize for what looks good, instead of what actually fulfills human values.</li>
<li>TODO</li>
</ul></ul><div>Would you agree that highly intelligent systems might fail to optimize exactly what their designers intended them to, and this is dangerous?</div>
<div>&#10149; <a href='{{site.baseurl}}/arguments/test-before-deploying.html'>It’s no problem, we would test it before deploying</a></div>
<div>&#10149; <a href='{{site.baseurl}}/arguments/careful-with-that-reward-function.html'>It’s no problem, we would just be careful with our reward function</a></div>
<div>&#10149; <a href='{{site.baseurl}}/arguments/wouldnt-be-functional.html'>It’s probably impossible to develop AGI without solving this problem - so nothing bad will happen</a></div>
<div>&#10149; <a href='{{site.baseurl}}/arguments/would-inevitably-be-solved.html'>This problem would be inevitably solved in order to even develop AGI</a></div>
<div>&#10149; <a href='{{site.baseurl}}/arguments/alignment-advances-equally.html'>But as we build more capable systems, surely our understanding of how to align them will advance equally well</a></div>
<div>&#10149; <a href='{{site.baseurl}}/arguments/need-to-know-what-type.html'>Need to know what type of AGI before we can talk about safety</a></div>
<div>&#10149; <a href='{{site.baseurl}}/arguments/misuse-is-a-bigger-problem.html'>Misuse is a bigger problem</a></div>
<div>&#10149; <a href='{{site.baseurl}}/arguments/this-is-not-as-dangerous-as-other-global-risks.html'>This is not as dangerous as other global risks</a></div>
<div>&#10149; <a href='{{site.baseurl}}/arguments/humans-have-alignment-problems-too.html'>Humans have alignment problems too</a></div>
<div>&#10149; <a href='#feedback'>Send Feedback</a></div>
