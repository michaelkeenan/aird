---
layout: argument
title: "Threat Models - how might AGI be dangerous?"
breadcrumbs: Threat Models - how might AGI be dangerous?:threat-models
---
<ul><li>We’ve argued that it’s possible that advanced AI systems (with self-knowledge, and advanced planning capabilities) will not be aligned with the whole of human values, and moreover might pursue instrumental goals in the pursuit of whatever original goals they were programmed with. These instrumental goals may include self-preservation (preventing being turned off or the goals being changed), and acquisition of resources and power. But how does this actually threaten humans?</li>
<ul><li>Most immediately, humans consume resources, or might be actively trying to stop the AI from achieving its goals. One strategy the AI could follow is to stop or kill humans. There are plenty of ways to do that: deploying superpandemics, lethal autonomous weapons, or any other way a superintelligent system could develop.</li>
<li>It is probably futile to try and defend against every one of these scenarios. Instead, we need to prevent the AI from ever being misaligned to human values in the first place.</li>
</ul><li>If an AGI never exceeds human level intelligence, it could still have tremendous advantages over humans that would make it a serious danger:</li>
<ul><li>AI has the advantage that it can instantiate perfect copies of itself, and optimize for perfect collaboration.</li>
<ul><li>Note: A sufficiently advanced AI could spin off copies that are in perfect alignment with each other. This is because an advanced AI, unlike a human mind, can potentially have full access to its model or source code - and could be super-human in interpreting and modifying it. It could make changes to ensure optimum collaboration.</li>
</ul><li>A human level AI could potentially run 10, 100 or even 1000 times faster than a human brain - thus outmaneuvering human thinking. The speed difference in communication (human-human vs AI-AI) might be much higher than that.</li>
<li>A human level AI could use any kind of software that we can use… but much faster, as it is not bound to keyboard, mouse and visual output.</li>
<li>An AI that is on the average human-like will still be significantly better than humans in some areas, for example arithmetic and data mining.</li>
<li>A human level AI could improve itself by rewriting its source code and copying itself to newer & faster computers. Humans, on the other hand, only have relatively mild forms of self-improvement available.</li>
<li>With current AI training methods, the training process is quite expensive computationally - whereas after the model has been trained, evaluation is relatively quickly and requires fewer computational resources. That means, after training is finished, there might be a lot of unused hardware sitting around that could be used to run many copies.</li>
<li>A human level AI could do human-level economic work and use the money earned to buy more hardware - and run even more instances on that.</li>
<li>Imagine a huge population of AIs, which are at their core copies of the same system - and thus perfectly cooperating with each other. Each one could, after specialized training, become quite skilled in some area and earn huge amounts of money. After some time, their virtual economy might be as big as the human one.</li>
<li>This would be a workforce of potentially millions of copies of a human-level generally intelligent agent, which are perfectly aligned with each other and can collaborate instantly on any plan.</li>
</ul><li>If we assume the AI could become more intelligent than humans, it could try a wide variety of strategies to defeat us:</li>
<ul><li>Hack into human-built software everywhere (even hackers of human level intelligence are able to hack software some of the time)</li>
<li>Manipulate human psychology (even humans can do that now, see for example the social media manipulation campaigns funded by state actors)</li>
<li>Quickly generate vast wealth through providing services, starting digital companies, the stock market or fraud.</li>
<li>Come up with better plans than humans and thus, make sure that human attempts to stop the AI would fail.</li>
<li>Develop advanced weaponry that can be quickly deployed and is powerful enough to defeat human militaries.</li>
</ul><li>Here are some stories on how this might play out:</li>
<ul><li>People might have given the AGI broad control over industrial systems, cities, cars, etc. The AGI might appear as if it is aligned with humanity's intentions, when in actuality something has gone wrong in the specification of its utility function, and it’s trying to deceive humanity into trusting it. The AI might behave as intended by its operators until some point in time, when it judges itself to have amassed enough power and changes its behavior.</li>
<li>There could be a collaboration between many different AI-operated companies, building an economy that is far more efficient than the human one, and which humans will grow to depend on. This network of companies could gain more and more power until humans are completely dependent on it. We may then realize that these companies follow their own incentives and do not have humanities best interests in mind. By that time they might be so entrenched, intertwined with our basic needs and well-defended that we cannot stop them. Eventually, resources necessary for human survival might become depleted. <a href='https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic' target='_blank'>This article</a> outlines this scenario and others along similar lines.</li>
</ul><li>In surveys, at least some AI researchers agree that a catastrophic scenario is possible. For the <a href='https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/#Extinction_from_human_failure_to_control_AI' target='_blank'>2022 expert survey on progress in AI</a>, 4271 researchers who published at NeurIPS or ICML were interviewed.</li>
<ul><li>When asked “What probability do you put on future AI advances causing human extinction or similarly permanent and severe disempowerment of the human species?”, the median response was 5%.</li>
<li>On “What probability do you put on human inability to control future advanced AI systems causing human extinction or similarly permanent and severe disempowerment of the human species?”, the median answer was 10%.</li>
</ul></ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
