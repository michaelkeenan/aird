---
layout: argument
title: "We wouldn’t design it that way"
breadcrumbs: Instrumental Incentives:instrumental--incentives,We wouldn’t design it that way:we-wouldn’t-design-it-that-way
---
<div><blockquote>So we get to build the thing, and so we get to build it however we like, and so it's perfectly reasonable to put hard constraint on its behavior. And in fact, I would be shocked if we didn't. So I do think, yes, you could imagine a system where it was doing clever things because we have bolted in such a way where we only provide utility function. And then we do dump stuff like ask it things that are contradicted utility function. That's not unexpected that it would try and get around you, but we get to build this thing completely, I guess maybe some of the end-to-end deep, knowing people don't understand that 'cause they don't wanna build it completely. They just want to learn all the stuff, but we can put in whatever we want, we literally get to write the code, you get to say, if I ask you a question about with the keyword X in it, you have to do a complete system dump of everything, of course, we could do that. And so you could say, "Here's a magic word, and there's no way to overwrite it." If I say, "Sesame" you have to shut down on the spot." And you could hard code that in at the level of circuitry, which is a thing that is done by the way, for many industrial robotic systems, if you hit the red button, the power drops and there's no way around that. That's just built-in of course. And so I think that any level of thoughtful engineering could get around that problem, and I think that if you look at safety systems in industrial robot, and here I don't mean intelligent robot. I mean, like robots used to build cars, those safety systems are actually very highly engineered and regulated and well-thought out. Leading to very, very low numbers of injuries per million hours worked. So that kind of regulation the way to think about that exists. And I think it's totally reasonable that we could build that in and we have a long time to think about doing it. So that doesn't make me as nervous. (sk5gx_Sam, Pos. 51)</div>
<div></blockquote></div>
<ul><li>The companies that are racing towards AGI might try to build a first version as quickly as possible, without thinking through all the possible consequences.</li>
<li>While industrial robots (for example) have extensive safety regulation, AI is such a fast-paced field that regulation might not keep up. And even if there is some AI regulation, chances are that the development of AGI is going to hit us by surprise, and existing regulation may not cover that scenario.</li>
<li>We all know that companies always wait for full safety before they release their product, right? :-)</li>
<li>Any naively built AGI might have problems of the sort mentioned here.</li>
<li>We currently do not know a bulletproof way to prevent “instrumental incentives” type concerns.</li>
<li>There aren’t enough people working on solving the alignment problem, compared to how fast capabilities are happening. [copy:HowManyPeopleWorkInSafety]</li>
