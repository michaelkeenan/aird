---
layout: argument
title: "It’s probably impossible to develop AGI without solving this problem - so nothing bad will happen"
breadcrumbs: The Alignment Problem:the-alignment-problem,AGI wouldn’t be functional unless this problem is solved:wouldnt-be-functional
---
<div>
<blockquote>
The premise doesn’t make sense 
</blockquote>
</div>
<ul><li>That seems unlikely. Why would the AGI be nonfunctional unless the goals encompassed “everything we care about”? Surely we are going to try and build more and more complex AI systems, with various goals.</li>
<li>The goals will get more complex - and we might not know what we’re doing as we scale this up.</li>
