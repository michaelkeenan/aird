---
layout: argument
title: "We wouldn’t design it that way"
breadcrumbs: Instrumental Incentives:instrumental-incentives,We wouldn’t design it that way:wouldnt-design-that-way
---

<blockquote>
So we get to build the thing, and we get to build it however we like, and so it's perfectly reasonable to put hard constraints on its behavior. And in fact, I would be shocked if we didn't. So I do think, yes, you could imagine a system where it was doing clever things because we have built it in such a way where we only provide a utility function. And then we do dumb stuff, like ask it things that contradict its utility function. Then it’s not unexpected that it would try and get around you. But we get to build this thing completely. I guess maybe some of the end-to-end deep learning people don't understand that, because they don't want to build it completely. They just want to learn all the stuff. But we can put in whatever we want, we literally get to write the code. You get to say, “if I ask you a question with the keyword X in it, you have to do a complete system dump of everything.” Of course, we could do that. And so you could say, "Here's a magic word, and there's no way to overwrite it. If I say ‘sesame’ you have to shut down on the spot." And you could hard-code that in at the level of circuitry – which is a thing that is done by the way, for many industrial robotic systems. If you hit the red button, the power drops and there's no way around that. That's just built-in, of course. And so I think that any level of thoughtful engineering could get around that problem. And I think that if you look at safety systems in industrial robotics – and here I don't mean intelligent robots, I mean like robots used to build cars – those safety systems are actually very highly engineered and regulated and well thought out. Leading to very, very low numbers of injuries per million hours worked. So that kind of regulation, the way to think about that, exists. And I think it's totally reasonable that we could build that in, and we have a long time to think about doing it. So that doesn't make me as nervous. 
</blockquote>

<ul><li>The companies that are racing towards AGI might try to build a first version as quickly as possible, without thinking through all the possible consequences.</li>
<li>While industrial robots (for example) have extensive safety regulation, AI is such a fast-paced field that regulation might not keep up. And even if there is some AI regulation, chances are that the development of AGI is going to hit us by surprise, and existing regulation may not cover that scenario.</li>
<li>We all know that companies always wait for full safety before they release their product, right? :-)</li>
<li>Any naively built AGI might have problems of the sort mentioned here.</li>
<li>We currently do not know a bulletproof way to prevent “instrumental incentives” type concerns.</li>
<li>There aren’t enough people working on solving the alignment problem, compared to how fast capabilities are happening. <br/><li>At the moment, very few people work on the safety of future general AI systems (as opposed to improving ethical alignment of current narrow systems). It might be only around 300, while 10-100x as many people work on speeding up the progress towards general AI:<br/>see <a href='https://twitter.com/ben_j_todd/status/1489985966714544134?lang=en' target='_blank'>one estimate</a>, <a href='https://80000hours.org/problem-profiles/artificial-intelligence/' target='_blank'>another one</a> Obviously, there is no proof that TAI will cause catastrophe. We are not dealing with certainties here. But there are arguments from multiple directions indicating that there is at least some level of risk, and the consequences might be catastrophic. For a technology with such impacts, it seems like safety is grossly neglected. There is no reason to expect the safety community to suddenly grow by itself.<br/></li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
