<div class="page-data" data-page-title="Instrumental Incentives"></div><ul><li>Let's go back to the situation of a CEO AI running a company. Recall that it is difficult to put the goals we would want it to have into mathematical formulations.</li>
<li>There’s another problem that [replace this with what we discussed at the call]<br/>Suppose we have told the CEO AI</li>
<li>So it would try to find a way to prevent that from happening. For example, it could omit critical information which its human operators might find objectionable from its next report.</li>
<li>Such behaviour does not need to be programmed in. It would emerge under certain conditions: If the system is capable enough, models the world reasonably well and optimizes for almost any goal, it will want to prevent attempts to be shut down.</li>
<li>This does not require the AI system to care about “itself” in the sense that humans care about their own survival. All it requires is the understanding that its goals are unachievable if the system is turned off.</li>
<li>Furthermore: In order to solve problems faster and achieve its goals with higher likelihood, the AI system will try to gather resources and gain more power.</li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
