<div class="page-data" data-page-title="Misuse is a bigger problem"></div>
<blockquote>
I guess like anything I see, there is a risk, just like any other tool, that this will be used for nefarious purposes, like automated systems that make decisions in a way that we don't quite understand. Then baking in of bias, whether we're talking about automated weapon systems that make decisions on closing the loop in deciding who to shoot to automated decisions and policing and things like that. 
</blockquote>

<ul><li>Misuse continues to be a problem as we get more and more powerful systems. A great example of this is nuclear weapons: It used to be impossible for a small number of people to cause incredible damage. But now whoever is in control of those weapons has a huge amount of power to affect the world in a way that wasn’t true historically.</li>
<ul><li>You might argue that AI is similar, in that a small number of actors may have an outsized ability to affect the world. This is because the development of cutting-edge AI is currently happening at just a small number of well-funded companies. These companies will then have outsized power.</li>
<li>Alternatively, if it turns out that AGI is developed in a decentralized fashion, this will create misuse problems of its own.</li>
</ul><li>At least humanity has dealt with misuse before. Technological misuse has been very bad in the past when leaders have directed an outsized amount of resources, and much attention should be focused on this.</li>
<li>But we also want to pay attention to amore neglected risk that could potentially be even worse than misuse. The thing that we’re creating (AGI) takes an aspect of humanity that has enabled us to shape most of our environment and become earth’s dominant species - intelligence - and amps that up beyond human capabilities. Which means that the danger from misaligned superintelligent AI could be significantly higher than the danger from previous misuses of technology.</li>
</ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
