<div class="page-data" data-page-title="Nothing we can do at the moment"></div><p>Some argue that future AGI will use techniques we don’t know about yet. Therefore, it would be useless to try and work on safety now.</p>
<ul><li>Some AI safety researchers consider it plausible that AGI will be developed mostly using present-day techniques, and that it will happen soon. In that case, there seems to be a very strong case that safety techniques should be researched now, building upon the capabilities we do have (even if they do not constitute “general” intelligence yet). Furthermore, work in AI alignment helps us understand current systems better and thus use them more responsibly and effectively.</li>
<li>While future AGI will use future techniques, it is possible these will have some relation to AI systems currently in use. Therefore, if we develop alignment strategies that work on current systems, we could lay valuable groundwork for the alignment of future systems.</li>
<li>Much present-day work on AI safety is only weakly dependent on the technical details of the architecture of AGI. (See below for some examples of work currently being done). Therefore, our ignorance about these details is not a huge problem. Additionally, the field of AI safety research is really broad and covers many different approaches. It’s likely that at least some of them will be quite useful—even in the future with new techniques.</li>
<ul><li>Any progress on alignment, even without knowing how future AGI is built, would be a big step in the right direction. In the words of AI safety researcher <a href='https://ai-alignment.com/prosaic-ai-control-b959644d79c2' target='_blank'>Paul Cristiano writes</a>: “For now, finding any plausible approach to alignment, that works for any setting of unknown details, would be a big accomplishment. With such an approach in hand we could start to ask how sensitive it is to the unknown details, but it seems premature to be pessimistic before even taking that first step.”</li>
<li>Mechanistic interpretability is an attempt to reverse engineer the detailed computations performed by a model. In the past, this has been done mostly on CNN vision models. Researchers at AI safety research company Anthropic have made progress on <a href='https://transformer-circuits.pub/2021/framework/index.html' target='_blank'>mechanistic interpretability for Transformer language models (Olsson et al., 2022)</a>. David Bau’s lab at Northeastern University has developed methods for finding specific factual associations in transformer language models and then editing them (<a href='https://arxiv.org/abs/2202.05262' target='_blank'>Meng et al., 2022</a>).</li>
<li>Redwood Research is working on <a href='https://arxiv.org/abs/2205.01663' target='_blank'>reliability through adversarial training</a>, on a state-of-the-art language model.</li>
<li>Leading AI companies <a href='https://openai.com/blog/our-approach-to-alignment-research/' target='_blank'>OpenAI</a> and <a href='https://deepmindsafetyresearch.medium.com/' target='_blank'>DeepMind</a> have teams working on different alignment research approaches. There are also academics and academic groups like <a href='https://humancompatible.ai/research' target='_blank'>CHAI</a>, the <a href='https://wp.nyu.edu/arg/' target='_blank'>NYU Alignment Research Group</a>, Dan <a href='https://people.eecs.berkeley.edu/~hendrycks/' target='_blank'>Hendrycks</a> and <a href='https://jsteinhardt.stat.berkeley.edu/' target='_blank'>Jacob Steinhardt</a> at UC Berkeley, and <a href='https://www.davidscottkrueger.com/' target='_blank'>David Krueger’s group</a> at the University of Cambridge working on different approaches.</li>
<a href='../resources.html' ><li>More resources</a></li>
</ul><li>Currently, there are so few people working on AI safety that it seems worthwhile to increase the effort quite a bit , considering the magnitude of the problem. Also, consider how important AI will become over the next decades even before AGI is invented. <br/><li>At the moment, very few people are working on the safety of future general AI systems (as opposed to improving ethical alignment of current narrow systems). It might be only around 300, while 10-100x as many people work on speeding up the progress towards general AI (see <a href='https://twitter.com/ben_j_todd/status/1489985966714544134?lang=en' target='_blank'>one estimate here</a>, <a href='https://80000hours.org/problem-profiles/artificial-intelligence/' target='_blank'>another one here</a>). Obviously, there is no proof that advanced AI systems will cause catastrophe. We are not dealing with certainties here. But there are arguments from multiple directions indicating that there is at least some level of risk, and the consequences might be catastrophic. For a technology with such impacts, it seems like safety research is neglected.<br/></li>
<li>If research actually shows that AI alignment is not possible, then that is very important to know. In that case, we would need to rely on coordinating around not creating AGI at all to avoid catastrophe.</li>
</ul><p>Further Reading:</p>

<a href='https://arxiv.org/abs/2209.00626' target='_blank'>“The alignment problem from a deep learning perspective”</a> by Richard Ngo et al
<a href='https://ai-alignment.com/prosaic-ai-control-b959644d79c2' target='_blank'>“Prosaic AI Alignment”</a> by AI safety researcher Paul Cristiano 
<a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
