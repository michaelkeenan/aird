<div class="page-data" data-page-title="The Alignment Problem"></div><ul><li>Current ML systems can fail in surprising and unexpected ways. <a href='https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml' target='_blank'>Many examples have been collected</a></li>
<li>Suppose we have a future “CEO AI” that’s running a company. Their board of directors might give the AI the goal: “Maximize profits without exploiting people, don’t run out of money, and avoid side effects that people would consider objectionable”.</li>
<li>Currently we find it very challenging to translate these human values, preferences and intentions into mathematical formulations that can be optimized by systems. This might continue to be a problem in the future.</li>
<li>Mistakes in goal formulation could be dangerous and have far-reaching consequences. For example:</li>
<ul><li>The AI might optimize for what looks good, instead of what actually fulfills human values.</li>
<li>TODO</li>
</ul></ul><a name='argnav'/>
<br/><br/><br/><!-- temporary fix for issue 19 -->
<div><em class="question">Would you agree that highly intelligent systems might fail to optimize exactly what their designers intended them to, and this is dangerous?</em></div>
<div></div>
<div>&#10149; <a href='test-before-deploying'>It’s no problem, we would test it before deploying</a></div>
<div>&#10149; <a href='careful-with-that-reward-function'>It’s no problem, we would just be careful with our reward function</a></div>
<div>&#10149; <a href='wouldnt-be-functional'>It’s probably impossible to develop AGI without solving this problem - so nothing bad will happen</a></div>
<div>&#10149; <a href='would-inevitably-be-solved'>This problem would be inevitably solved in order to even develop AGI</a></div>
<div>&#10149; <a href='alignment-advances-equally'>But as we build more capable systems, surely our understanding of how to align them will advance equally well</a></div>
<div>&#10149; <a href='need-to-know-what-type'>Need to know what type of AGI before we can talk about safety</a></div>
<div>&#10149; <a href='misuse-is-a-bigger-problem'>Misuse is a bigger problem</a></div>
<div>&#10149; <a href='this-is-not-as-dangerous-as-other-global-risks'>This is not as dangerous as other global risks</a></div>
<div>&#10149; <a href='humans-have-alignment-problems-too'>Humans have alignment problems too</a></div>
<div>&#10149; <a href='instrumental-incentives'>OK, I agree there might be a problem here. Let’s move on.</a></div>
<div>&#9993; <a href='#feedback'>Send Feedback</a></div>
