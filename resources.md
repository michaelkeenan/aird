---
layout: page
title: Resources
image: assets/images/pic01.jpg
nav-menu: true
order: 3
---

<!-- Main -->
<div id="main" class="alt">

<!-- One -->
<section id="one">
	<div class="inner">
		<header class="major">
			<h1>Resources</h1>
		</header>

<!-- Content -->
<h2 id="content">Readings</h2>

<p> add an introduction </p>

<h2>Core readings for ML researchers</h2>
<h3>Arguments for risk from advanced AI systems</h3>
<ul>
    <li><a href="https://wp.nyu.edu/arg/why-ai-safety/">"Why I Think More NLP Researchers Should Engage with AI Safety Concerns"</a> by Sam Bowman (2022), 15m <i>(stop at the section "The new lab")</i></li>
    <li><a href="https://www.youtube.com/watch?v=yl2nlejBcg0">Researcher Perceptions of Current and Future AI"</a> by Vael Gates (2022), 48m <i>(skip the Q&A)</i></li>
</ul>
<h3>Orienting</h3>
<ul>
    <li><a href="https://bounded-regret.ghost.io/more-is-different-for-ai/">"More is Different for AI"</a> by Jacob Steinhardt (2022), 30m <i>(intro and first three posts only)</i></li>
    <li><a href="https://docs.google.com/document/d/1j7tZ1Xf7-l2k2qr2t3MFwi-IkhXNdzA2N2WZBfcghsM/edit?usp=sharing">AI Timelines/Risk Projections as of Sep. 2022"</a><i>(first 3 pages only)</i>, 5m</li>
    <li><a href="https://www.alignmentforum.org/posts/6ccG9i5cTncebmhsH/frequent-arguments-about-alignment">​​Frequent Arguments About Alignment"</a> by John Schulman (2021), 15m</li>
</ul>
<h3>Research directions</h3>
<ul>
	<li><a href="https://arxiv.org/pdf/2209.00626.pdf">The Alignment Problem from a Deep Learning Perspective"</a> by Richard Ngo et al. (2022), 65m</li>
    <li>Watch <a href="https://www.youtube.com/watch?v=-vsYtevJ2bc">"Current Work in AI Alignment"</a> by Paul Christiano (2019), 30m (Note: here is the transcript)</li>
    <li>(these are much less vetted, added quickly) Research in <a href="https://arxiv.org/pdf/2210.01790.pdf">goal misgeneralization</a> (Shah, 2022); power-seeking (<a href="https://proceedings.neurips.cc/paper/2021/file/c26820b8a4c1b3c2aa868d6d57e14a79-Paper.pdf">Turner, 2021</a>); <a href="https://deepmindsafetyresearch.medium.com/specification-gaming-the-flip-side-of-ai-ingenuity-c85bdb0deeb4">specification gaming (Krakovna, 2020)</a>; mechanistic interpretability (<a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Olsson et al. (2022)</a>, <a href="https://arxiv.org/abs/2202.05262">Meng K., et al. (2022)</a>); eliciting latent knowledge (<a href="https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit">ELK</a>); ML safety divided into robustness, monitoring, alignment and external safety <a href="https://arxiv.org/pdf/2109.13916.pdf">(Hendrycks, 2022)</a></li>
</ul>
 

<h2>Core readings for the public</h2>
<ul>
	<li><a href="https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment">"The Case For Taking AI Seriously As A Threat to Humanity"</a> by Kelsey Piper (2020), 30m</li>
    <li><a href="https://smile.amazon.com/Alignment-Problem-Machine-Learning-Values-ebook/dp/B085T55LGK/">"The Alignment Problem"</a> by Brian Christian (2020), book</li>
</ul>


<h2> NEXT STEPS </h2>

<p> FILL </p>

<h2> Book a call with me? </h2>

<p> Email or a form?</p>

</div>
</section>

</div>
