---
layout: page
title: Resources
image: assets/images/pic01.jpg
nav-menu: true
order: 3
---

<!-- Main -->
<div id="main" class="alt">

<!-- One -->
<section id="one">
	<div class="inner">
		<header class="major">
			<h1>Resources</h1>
		</header>

<!-- Content -->
<!-- <h2 id="content">Readings</h2> -->

<!-- <p><i> Sorted by <a href="#MLreadings">machine learning researchers</a>, <a href="#publicreadings">general audience</a>, and <a href="what_can_i_do.html">more involved.</a> </i></p> -->

<h3 id="MLreadings">For machine learning researchers</h3>
<div class="row">
	<div class="6u 12u$(medium)">
		<div class = "box">
		<h4>Risks from advanced AI</h4>
		<ul>
		    <li><a href="https://wp.nyu.edu/arg/why-ai-safety/" class="button xsmall">∗</a> <a href="https://wp.nyu.edu/arg/why-ai-safety/"> Why I Think More NLP Researchers Should Engage with AI Safety Concerns</a> by Sam Bowman (2022) <!-- , 15m <i>(stop at the section "The new lab")</i> --> </li>
		    <li><a href="https://www.youtube.com/watch?v=yl2nlejBcg0" class="button xsmall">∗</a> <a href="https://www.youtube.com/watch?v=yl2nlejBcg0"> Researcher Perceptions of Current and Future AI</a> by Vael Gates (2022)<!-- , 48m <i>(skip the Q&A)</i> --> </li>
		</ul>
		</div>
	</div>
	<div class="6u$ 12u$(medium)">
		<div class = "box">
		<h4>Orienting</h4>
		<ul>
		    <li><a href="https://bounded-regret.ghost.io/more-is-different-for-ai/" class="button xsmall">∗</a> <a href="https://bounded-regret.ghost.io/more-is-different-for-ai/"> More is Different for AI</a> by Jacob Steinhardt (2022)</li>
		    <li><a href="https://docs.google.com/document/d/1j7tZ1Xf7-l2k2qr2t3MFwi-IkhXNdzA2N2WZBfcghsM/edit?usp=sharing">AI Timelines/Risk Projections as of Sep. 2022</a><!-- <i>(first 3 pages only)</i>, 5m --></li>
		    <li><a href="https://www.alignmentforum.org/posts/6ccG9i5cTncebmhsH/frequent-arguments-about-alignment">​​Frequent Arguments About Alignment</a> by John Schulman (2021)<!-- , 15m --></li>
		</ul>
		</div>
	</div>
</div>
<div class = "box">
<h4>Research directions</h4>
<h5> Reviews </h5>
<ul>
	<li><a href="https://arxiv.org/pdf/2209.00626.pdf" class="button xsmall">∗</a> <a href="https://arxiv.org/pdf/2209.00626.pdf"> The Alignment Problem from a Deep Learning Perspective</a> by Richard Ngo et al. (2022)<!-- , 65m --></li>
	<li><a href="https://arxiv.org/pdf/2109.13916.pdf"> Unsolved Problems in ML Safety</a> by Hendrycks et al. (2022)</li>
    <li><a href="https://www.youtube.com/watch?v=-vsYtevJ2bc"> Current Work in AI Alignment</a> by Paul Christiano (2019)<!-- , 30m (<a href="https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment">transcript</a>) --></li>
</ul>
<h5> Primary </h5>
<ul>
    <li><a href="https://arxiv.org/pdf/2210.01790.pdf" class="button xsmall">∗</a> <a href="https://arxiv.org/pdf/2210.01790.pdf"> Goal Misgeneralization: Why correct Specifications Aren't Enough For Correct Goals</a> (Shah et al., 2022)</li>
    <li><a href="https://deepmindsafetyresearch.medium.com/specification-gaming-the-flip-side-of-ai-ingenuity-c85bdb0deeb4">Specification gaming: the flip side of AI ingenuity </a> (Krakovna et al., 2020)</li>
    <li>Mechanistic interpretability <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">(Olsson et al., 2022)</a>, <a href="https://arxiv.org/abs/2202.05262">(Meng et al. 2022)</a></li> 
    <li><a href="https://proceedings.neurips.cc/paper/2021/file/c26820b8a4c1b3c2aa868d6d57e14a79-Paper.pdf">Optimal Policies Tend to Seek Power</a> (Turner et al., 2021)</li>
    <!-- <li>Eliciting latent knowledge (<a href="https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit">ELK</a> </li>-->
</ul>
</div>
<div class = "box">
<h4> Further resources </h4>
<p>More extensive, in-depth materials located in <a href="what_can_i_do#further_resources">What can I do?</a></p>
</div>

<br>

<h3 id="publicreadings">For a general audience</h3>
<ul>
	<li><a href="https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment" class="button xsmall">∗</a><a href="https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment"> The Case For Taking AI Seriously As A Threat to Humanity</a> by Kelsey Piper (2020)<!-- , 30m --></li>
    <li><a href="https://smile.amazon.com/Alignment-Problem-Machine-Learning-Values-ebook/dp/B085T55LGK/"> The Alignment Problem </a> by Brian Christian (2020)<!-- , book --></li>
    <li><a href="https://www.youtube.com/watch?v=UbruBnv3pZU"> Existential Risk from Power-Seeking AI</a> by Joe Carlsmith (2021)</li>
    <li><a href="https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/">Why AI Alignment Could be Hard with Modern Deep Learning</a> by Ajeya Cotra (2021)</li>
    <li><a href="https://80000hours.org/problem-profiles/artificial-intelligence/">80,000 Hours Podcast: Preventing an AI-related Catastrophe</a> (2022)<!-- , 2.5h --></li>
	<li><a href="https://www.cold-takes.com/most-important-century/">The Most Important Century</a> by Holden Karnofsky (podcast, summaries, various articles)</li>
	<li><a href="https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg/">AI Safety YouTube channel</a> by Robert Miles</li>
</ul>

</div>
</section>

</div>
